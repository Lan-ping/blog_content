卷积和自注意力是表示学习的两种强大技术，它们通常被认为是两种截然不同的对等方法。在本文中，我们表明它们之间存在很强的潜在关系，因为这两种范式的大部分计算实际上是通过相同的操作完成的。具体来说，我们首先展示了内核大小为 k×k 的传统卷积可以分解为 $k^2$ 个单独的 1×1 卷积，然后进行移位和求和操作。然后，我们将 self-attention 模块中查询、键和值的投影解释为多个 1×1 卷积，并计算注意力权重和值的聚合。因此，两个模块的第一阶段都包含类似的操作。更重要的是，与第二阶段相比，第一阶段贡献了主要的计算复杂度（通道大小的平方）。这一观察自然地导致了这两种看似不同的范式的优雅整合，即一个混合模型，它享有自注意力和卷积 (ACmix) 的好处，同时与纯卷积或自注意力对应物相比具有最小的计算开销.大量实验表明，我们的模型在图像识别和下游任务的竞争基线上取得了持续改进的结果。代码和预训练模型将在 https://github.com/Panxuran/ACmix 和 https://gitee.com/mindspore/models 发布。

# 1. 引言

近年来见证了计算机视觉中卷积和自注意力的巨大发展。 卷积神经网络 (CNN) 被广泛应用于图像识别 [20, 24]、语义分割 [9] 和对象检测 [39]，并在各种基准测试中实现了最先进的性能。 另一方面，self-attention 最早被引入自然语言处理 [1, 43]，在图像生成和超分辨率领域也显示出巨大的潜力 [10, 35]。 最近，随着 ViT [7,16,38] 的出现，基于注意力的模块在许多视觉任务上取得了与 CNN 同类产品相当甚至更好的性能。

尽管这两种方法都取得了巨大的成功，但卷积和自注意力模块通常遵循不同的设计范式。 传统卷积根据卷积滤波器权重在局部感受野上利用聚合函数，这些权重在整个特征图中共享。 内在特性对图像处理施加了至关重要的感应偏差。 相比之下，自注意力模块基于输入特征的上下文应用加权平均操作，其中注意力权重是通过相关像素对之间的相似度函数动态计算的。 这种灵活性使注意力模块能够自适应地关注不同的区域并捕获更多信息特征。

考虑到卷积和自注意力的不同和互补特性，通过集成这些模块，有可能从这两种范式中受益。 之前的工作从几个不同的角度探索了自注意力和卷积的结合。 早期阶段的研究，例如 SENet [23]、CBAM [47]，表明自注意力机制可以作为卷积模块的增强。 最近，自注意力模块被提出作为单独的块来替代 CNN 模型中的传统卷积，例如 SAN [54]、BoTNet [41]。 另一类研究侧重于在单个块中结合自注意力和卷积，例如 AA-ResNet [3]、Container [17]，而架构在为每个模块设计独立路径方面受到限制。 因此，现有方法仍然将 selfattention 和卷积视为不同的部分，并且它们之间的潜在关系尚未得到充分利用。

在本文中，我们试图挖掘自注意力和卷积之间更密切的关系。 通过分解这两个模块的操作，我们表明它们严重依赖于相同的 1×1 卷积操作。 基于这一观察，我们开发了一个名为 ACmix 的混合模型，并以最小的计算开销优雅地集成了自注意力和卷积。 具体来说，我们首先用 1×1 卷积投影输入特征图，并获得一组丰富的中间特征。 然后，按照不同的范式，即分别以自注意和卷积方式重用和聚合中间特征。 这样一来，ACmix 就可以享受到两个模块的优势，并且有效地避免了进行两次昂贵的投影操作。

总而言之，我们的贡献有两个方面：
1. 揭示了自我注意和卷积之间的强烈潜在关系，为理解两个模块之间的联系提供了新的视角，并为设计新的学习范式提供了灵感。
2. 提出了一个优雅的自注意力和卷积模块的集成，它享有两全其美的好处。 经验证据表明，混合模型始终优于其纯卷积或自注意力模型。

# 2. 相关工作

使用卷积核提取局部特征的卷积神经网络 [27, 28] 已成为各种视觉任务 [20, 25, 40] 中最强大和最常规的技术。 同时，self-attention 也展示了它在广泛的语言任务（如 BERT 和 GPT3 [4,14,37]）上的普遍表现。 理论分析[11]表明，当配备足够大的容量时，self-attention可以表达任何卷积层的函数类。 因此，最近的一项研究探索了将自注意力机制应用于视觉任务的可能性 [16, 23]。 有两种主流方法，一种使用自注意力作为网络中的构建块 [7,33,55]，另一种将自注意力和卷积视为互补部分 [6,29,45]。

## 2.1 Self-Attention only

受自注意力在远程依赖中的表达能力的启发 [14, 43]，一系列工作努力仅使用自注意力作为基本构建块来构建视觉任务模型 [2,7,16] . 一些作品 [38, 54] 表明，自注意力可以成为完全替代卷积操作的视觉模型的独立原语。 最近，Vision Transformer [16] 表明，给定足够的数据，我们可以将图像视为 256 个标记的序列，并利用 Transformer 模型 [43] 在图像识别方面取得有竞争力的结果。 此外，在检测 [2,7,57]、分割 [46,53,55]、点云识别 [18,33] 和其他视觉任务 [8,35] 中采用了 transformer 范式。

## 2.2 注意力增强卷积

多个先前提出的图像注意机制表明它可以克服卷积网络的局部性限制。因此，许多研究人员探索使用注意力模块或利用更多关系信息来增强卷积网络功能的可能性。特别是，Squeeze-andExcitation (SE) [23] 和 Gather-Excite (GE) [22] 重新加权每个通道的映射。 BAM [34] 和 CBAM [47] 独立地重新加权通道和空间位置，以更好地细化特征图。 AA-Resnet [3] 通过连接来自另一个独立的自注意力管道的注意力图来增强某些卷积层。 BoTNet [41] 在模型的后期用自注意力模块代替卷积。一些工作旨在通过聚合来自更广泛像素的信息来设计更灵活的特征提取器。Hu et al.  [21]提出了一种局部关系方法，根据局部像素的组成关系自适应地确定聚合权重。Wang et al. 提出了非局部网络[45]，它通过引入比较全局像素之间相似性的非局部块来增加感受野。

## 2.3 卷积增强注意力

随着Vision Transformer  [16] 的出现，已经提出了许多基于 transformer 的变体，并在计算机视觉任务上取得了显着的改进。 其中存在的研究侧重于用卷积运算补充 transformer 模型以引入额外的归纳偏差。 CvT [48] 在标记化过程中采用卷积，并利用步幅卷积来降低自注意力的计算复杂度。 ViT with convolutional stem [50] 提出在早期阶段添加卷积以实现更稳定的训练。 CSwin Transformer [15] 采用基于卷积的位置编码技术，并显示出对下游任务的改进。 Conformer [36] 将 Transformer 与独立的 CNN 模型相结合，以整合这两个特征。

# 3. 重温卷积和自注意力

卷积和自注意力以其当前形式广为人知。 为了更好地捕捉这两个模块之间的关系，我们通过将它们的操作分解为单独的阶段来从新的角度重新审视它们。

## 3.1 卷积

卷积是现代卷积网络中最重要的部分之一。 我们首先回顾标准的卷积操作，并从不同的角度对其进行重新表述。 图解如图 2 (a) 所示。 为简单起见，我们假设卷积的步长为 1。

考虑一个带有核 $K \in R^{C_{out} \times C_{in} \times k \times k}$ 的标准卷积，其中 $k$ 是核大小，$C_{in}$,  $C_{out}$ 是输入和输出通道的大小。 给定张量 $F \in R^{C_{in} \times H \times W}$，$G \in R^{C_{out} \times H \times W}$ 分别作为输入和输出的特征图，其中$H$，$W$表示高度和宽度，我们表示$f_{i,j} \in R^{C_{in}}$， $g_{i,j} \in R^{C_{out}}$ 作为像素 $(i, j)$ 的特征张量，分别对应于 $F$ 和 $G$。 然后，标准卷积可以表示为：

$$
\tag{1}
g_{i,j} = \sum_{p,q} K_{p,q}f_{i+p-\left \lfloor  k /2 \right \rfloor, j+q-\left \lfloor  k /2 \right \rfloor}
$$

其中，其中 $K_{p,q} \in R^{C_{out} \times C_{in}}$，表示关于内核位置 $(p,q)$ 的索引的内核权重。

为方便起见，我们可以将公式 (1) 重写为来自不同内核位置的特征图的总和：

$$
\tag{2}
g_{i,j} = \sum_{p,q} g_{i,j}^{p,q}
$$

和

$$
\tag{3}
g_{i,j}^{p,q} =  K_{p,q}f_{i+p-\left \lfloor  k /2 \right \rfloor, j+q-\left \lfloor  k /2 \right \rfloor}
$$

为了进一步简化公式，我们将 Shift 操作  定义为：
$$
\tag{4}
\tilde{f}_{i,j}=f_{i+\Delta x,j+\Delta y}, \forall i,j
$$

其中 $\Delta x$，$\Delta y$ 对应于水平和垂直位移。 那么，公式 (3) 式可以改写为：

$$
\tag{5}
\begin{aligned}
g_{i,j}^{p,q} &=  K_{p,q}f_{i+p-\left \lfloor  k /2 \right \rfloor, j+q-\left \lfloor  k /2 \right \rfloor}\\
                   &= \textrm{Shift}(K_{p,q}f_{i,j}, p-\left \lfloor  k /2 \right \rfloor, q-\left \lfloor  k /2 \right \rfloor)
\end{aligned}
$$

因此，标准卷积可以概括为两个阶段：

**Stage I: **
$$
\tag{6}
\tilde{g}_{i,j}^{p,q} = K_{p,q}f_{i,j}
$$

**Stage II: **
$$
\tag{7}
\tilde{g}_{i,j}^{p,q} = \textrm{Shift}(\tilde{g}_{i,j}^{p,q} p-\left \lfloor  k /2 \right \rfloor, q-\left \lfloor  k /2 \right \rfloor)
$$

$$
\tag{8}
\tilde{g}_{i,j} = \sum_{p,q} g_{i,j}^{p,q}
$$

在第一阶段，输入特征图是线性投影 *w.r.t*。 来自某个位置的内核权重，即 $(p,q)$。 这与标准的 1 × 1 卷积相同。 而在第二阶段，投影的特征图根据内核位置移动并最终聚合在一起。 可以很容易地观察到，大部分计算成本都是在 1×1 卷积中执行的，而随后的移位和聚合是轻量级的。

## 3.2 自注意力机制

注意机制在视觉任务中也被广泛采用。 与传统的卷积相比，注意力允许模型专注于更大尺寸上下文中的重要区域。 我们在图 2 (b) 中展示了图示。

考虑一个具有 $N$ 个头的标准自注意力模块。 令 $F \in R^{C_{in} \times H \times W}$，$G \in R^{C_{out} \times H \times W}$ 表示输入和输出特征。 令 $f_{i,j} \in R^{C_{in}}$， $g_{i,j} \in R^{C_{out}}$ 表示像素 $(i, j)$ 的对应张量。 然后，注意力模块的输出计算为：

$$
\tag{9}
g_{i,j} = \parallel _{l=1}^{N}(\sum_{a,b\in \mathfrak{N}_k(i,j)}A(W_q^{(l)}f_{i,j},W_k^{(l)}f_{a,b})W_v^{(l)}f_{a,b})
$$

其中，$\parallel$ 是 $N$ 个注意力头的输出的拼接，$W_q^{(l)}$，$W_k^{(l)}$，$W_v^{(l)}$ 是查询、键和值的投影矩阵。 $\mathfrak{N}_k(i,j)$ 表示以 $(i, j)$ 为中心的空间范围为 $k$ 的像素的局部区域，$A(W_q^{(l)}f_{i,j},W_k^{(l)}f_{a,b})$ 是关于 $\mathfrak{N}_k(i,j)$ 内特征的相应注意力权重。

对于 [21,38] 中广泛采用的自注意力模块，注意力权重计算为：

$$
\tag{10}
A(W_q^{(l)}f_{i,j},W_k^{(l)}f_{a,b})=\underset{\mathfrak{N}_k(i,j)}{\textrm{softmax}}(\frac{(W_q^{(l)}f_{i,j})^\intercal(W_k^{(l)}f_{a,b})}{\sqrt{d}})
$$

其中 $d$ 是 $W_q^{(l)}f_{i,j}$ 的特征维度数。

同样，多头自注意力可以分解为两个阶段，并重新表述为：

**Stage I: **
$$
\tag{11}
q_{i,j}^{(l)}=W_q^{(l)}f_{i,j},\;k_{i,j}^{(l)}=W_k^{(l)}f_{i,j},\;v_{i,j}^{(l)}=W_v^{(l)}f_{i,j}
$$

**Stage II: **
$$
\tag{12}
g_{i,j} = \parallel _{l=1}^{N}(\sum_{a,b\in \mathfrak{N}_k(i,j)}A(q_{i,j}^{(l)},k_{a,b}^{(l)})v_{a,b})
$$

与第 3.1 节中的传统卷积类似，在第一阶段首先进行 1×1 卷积，将输入特征投影为查询、键和值。 另一方面，第二阶段包括注意力权重的计算和值矩阵的聚合，即收集局部特征。 与第一阶段相比，相应的计算成本也被证明是较小的，遵循与卷积相同的模式。

## 3.3 计算成本

为了充分理解卷积和自注意力模块的计算瓶颈，我们分析了每个阶段的浮点运算 (FLOP) 和参数数量，并在表 1 中进行了总结。 结果表明，卷积阶段 I 的理论 FLOP 和参数相对于通道大小 $C$ 具有二次复杂度，而阶段 II 的计算成本与 $C$ 成线性关系，并且不需要额外的训练参数。

自我注意模块也发现了类似的趋势，其中所有训练参数都保留在第一阶段。至于理论 FLOP，我们考虑了类 ResNet 模型中的正常情况，其中 $k_a = 7$ 和 $C$ = 64, 128, 256, 512 用于各种层。 明确表明，当 $3C^2>2k_a^2C$ 时，第一阶段消耗的操作量更大，并且随着通道大小的增加，差异更加明显。

为了进一步验证我们分析的有效性，我们还在 Tab.1 中总结了 ResNet50 模型中卷积和自注意力模块的实际计算成本。 我们实际上将所有 3×3 卷积（或自注意力）模块的成本加起来，以从模型的角度反映趋势。 结果表明，在第一阶段进行了 99% 的卷积计算和 83% 的自注意力，这与我们的理论分析一致。

# 4 Method

## 4.1 将自注意力机制与卷积联系起来

第 3 节中对 self-attention 和卷积模块的分解从各个角度揭示了更深层次的关系。 首先，这两个阶段扮演的角色非常相似。 第一阶段是一个特征学习模块，两种方法通过执行 1×1 卷积将特征投影到更深的空间来共享相同的操作。 另一方面，阶段 II 对应于特征聚合的过程，尽管它们的学习范式存在差异。

从计算的角度来看，在卷积和自注意力模块的第一阶段进行的 1×1 卷积需要理论 FLOP 的二次复杂度和关于通道大小 C 的参数。相比之下，在第二阶段，两个模块 轻量级或几乎无需计算。

作为结论，上述分析表明（1）卷积和自注意力在通过 1×1 卷积投影输入特征图时实际上共享相同的操作，这也是两个模块的计算开销。 (2) 虽然对于捕获语义特征至关重要，但第二阶段的聚合操作是轻量级的，并且不会获取额外的学习参数。

## 4.2 自注意力和卷积的整合

上述观察可以得出卷积和自我注意的优雅整合。 由于两个模块共享相同的 1×1 卷积操作，我们只能执行一次投影，并将这些中间特征图分别重用于不同的聚合操作。 我们提出的混合模块 ACmix 的图示如图 2(c) 所示。

具体来说，ACmix 也包括两个阶段。 在第一阶段，输入特征由三个 1×1 卷积投影并分别重塑为 $N$ 块。 因此，我们获得了一组丰富的包含 3×$N$ 特征图的中间特征。

在第二阶段，它们按照不同的范式使用。 对于自注意力路径，我们将中间特征收集到 $N$ 组中，其中每组包含三个特征，一个来自每个 1×1 卷积。 对应的三个特征图用作查询、键和值，遵循传统的多头自注意力模块（公式 (12)）。 对于内核大小为 $k$ 的卷积路径，我们采用轻全连接层并生成 $k^2$ 个特征图。 因此，通过移位和聚合生成的特征（等式 (7)， (8)），我们以卷积方式处理输入特征，并像传统的那样从本地感受野收集信息。

最后，两条路径的输出相加，强度由两个可学习的标量控制：

$$
\tag{13}
F_{out}=\alpha F_{att}+\beta F_{conv}
$$

## 4.3 改进的移位和求和

如第 4.2 节和图 2 所示，卷积路径中的中间特征遵循传统卷积模块中进行的移位和求和操作。 尽管它们在理论上是轻量级的，但将张量向各个方向移动实际上会破坏数据的局部性，并且难以实现矢量化实现。 这可能会极大地影响我们模块在推理时的实际效率。

作为一种补救措施，我们采用具有固定内核的深度卷积来替代低效的张量移位，如图 3 (b) 所示。 以 $\textrm{Shift}(f,-1,-1)$ 为例，移位后的特征计算为：

$$
\tag{14}
\tilde{f}_{c,i,j} = f_{c,i-1,j-1}, \forall c,i,j
$$

其中 $c$ 表示输入特征的每个通道。

另一方面，如果我们将卷积核（内核大小 k = 3）表示为：

$$
\tag{15}
K_c = \begin{bmatrix}
1 & 0 & 0 \\ 
0 & 0 & 0\\ 
0 & 0 & 0
\end{bmatrix}, \forall c
$$

相应的输出可以表示为：

$$
 \tag{16}
\begin{aligned}
f_{i,j}^{(\textrm{dwc})} &= \sum_{p,q\in \{0,1,2\}} K_{c,p,q}f_{c,i+p-\left \lfloor  k /2 \right \rfloor, j+q-\left \lfloor  k /2 \right \rfloor}\\
&=f_{c,i-1,j-1} = \tilde{f}_{c,i,j} 
\end{aligned}
$$

因此，通过针对特定移位方向精心设计的核权重，卷积输出等效于简单的张量移位（等式(14)） 为了进一步结合来自不同方向的特征的总和，我们分别连接所有输入特征和卷积核，并将移位操作表示为单组卷积，如图 3 (c.I) 所示。 这种修改使我们的模块具有更高的计算效率。

在此基础上，我们额外引入了一些适配来增强模块的灵活性。 如图 3 (c.II) 所示，我们将卷积核作为可学习的权重释放，并将移位核作为初始化。 这提高了模型容量，同时保持了原始班次操作的能力。 我们还使用多组卷积核来匹配卷积和自注意力路径的输出通道维度，如图 3 (c.III) 所示。

## 4.4 ACmix 的计算成本

为了更好的比较，我们在 Tab.1 中总结了 ACmix 的 FLOPs 和参数。 Stage I 的计算成本和训练参数与 self-attention 相同，并且比传统卷积（例如 3×3 conv）更轻。 在第二阶段，ACmix 引入了额外的计算开销，其中包含轻量级全连接层和第 4.3 节中描述的组卷积，其计算复杂度与通道大小 C 呈线性关系，与第一阶段相当小。 ResNet50 模型与理论分析显示出类似的趋势。

## 4.5 推广到其他注意力模式

随着自注意力机制的发展，许多研究都集中在探索注意力算子的变化以进一步提升模型性能。 [54] 提出的 Patchwise attention 结合了来自局部区域中所有特征的信息作为注意力权重来代替原始的 softmax 操作。 Swin-Transformer [32] 采用的窗口注意力在同一局部窗口中保持 Token 的相同感受野，以节省计算成本并实现快速推理速度。 另一方面，ViT 和 DeiT [16,42] 考虑全局注意在单层内保留长期依赖关系。 这些修改在特定的模型架构下被证明是有效的。

在这种情况下，值得注意的是，我们提出的 ACmix 独立于自注意公式，并且可以很容易地应用于上述变体。 具体来说，注意力权重可以总结为：

$$
\tag{17}
\mathbf{(Patchwise)} \; A(q_{i,j},k_{a,b})=\phi ([q_{i,j},[k_{a,b}]_{a,b\in\mathfrak{N}_{k}(i,j)})
$$

$$
\tag{18}
\mathbf{(Window)} A(q_{i,j},k_{a,b})=\underset{a,b\in \mathfrak{W}_k(i,j)}{\textrm{softmac}}(q_{i,j}^{\intercal}k_{a,b}/\sqrt{d})
$$

$$
\tag{19}
\mathbf{(Global)} \; A(q_{i,j},k_{a,b})=\underset{a,b\in \mathfrak{W}}{\textrm{softmac}}(q_{i,j}^{\intercal}k_{a,b}/\sqrt{d})
$$

其中 $[\cdot]$ 表示特征连接，$\phi$ 表示具有中间非线性激活的两个线性投影层，$\mathfrak{W}_k(i,j)$ 是每个查询 Token的专用感受野，$\mathfrak{W}$ 表示整个特征图。 然后，计算出的注意力权重可以应用于等式 (12) 适合一般公式。